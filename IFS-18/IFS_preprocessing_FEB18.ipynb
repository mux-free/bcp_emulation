{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import dask\n",
    "import tempfile\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('/home/freimax/msc_thesis/scripts/helpers/')\n",
    "from data_preprocessing_helpers import calculate_rh_ifs\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Function to retireve the numerical value of every month: DEC -> 12\n",
    "def get_numeric_month(month):\n",
    "    # Create a dictionary to map month strings to numerical representation\n",
    "    month_mapping = {\n",
    "        'JAN': '01',\n",
    "        'FEB': '02',\n",
    "        'MAR': '03',\n",
    "        'APR': '04',\n",
    "        'MAY': '05',\n",
    "        'JUN': '06',\n",
    "        'JUL': '07',\n",
    "        'AUG': '08',\n",
    "        'SEP': '09',\n",
    "        'OCT': '10',\n",
    "        'NOV': '11',\n",
    "        'DEC': '12' }\n",
    "    # Map the month string to numerical representation using the dictionary\n",
    "    num_month = month_mapping.get(month[0:3])\n",
    "    return num_month\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def load_data(month):\n",
    "    # define path names\n",
    "    ifs_path = f'/net/helium/atmosdyn/IFS-1Y/FEB18/cdf'\n",
    "    # define path names\n",
    "    ifs_path = f'/net/helium/atmosdyn/IFS-1Y/{month}/cdf'    \n",
    "    \n",
    "    if month == 'MAR18':\n",
    "        cyclone_mask = f'/net/helium/atmosdyn/IFS-1Y/{month}/features/tracking/CYCLONES_MAR13.nc'\n",
    "    elif month == \"OCT18\":\n",
    "        cyclone_mask = f'/net/helium/atmosdyn/IFS-1Y/{month}/features/tracking/CYCLONES.nc'\n",
    "    else:\n",
    "        cyclone_mask = f'/net/helium/atmosdyn/IFS-1Y/{month}/features/tracking/CYCLONES_{month}.nc'\n",
    "    # LOAD DATA\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "        # ds_p_ifs = xr.open_mfdataset(f'{ifs_path}/P201802*', drop_variables=['ttot','tdyn','tsw','tlw','tmix','tconv','tls','tcond','tdep','tbf','tevc','tfrz','trime','tce'])\n",
    "        # ds_s_ifs = xr.open_mfdataset(f'{ifs_path}/S201802*', drop_variables=['PS','PVRCONVT','PVRCONVM','PVRTURBT','PVRTURBM','PVRLS','PVRCOND','PVRSW','PVRLWH','PVRLWC','PVRDEP','PVREVC','PVREVR', 'PVRSUBI', 'PVRSUBS', 'PVRMELTI', 'PVRMELTS', 'PVRFRZ', 'PVRRIME','PVRBF','LABEL'])\n",
    "        ds_cycmask = xr.open_dataset(cyclone_mask)\n",
    "    import glob\n",
    "    ifs_path = f'/net/helium/atmosdyn/IFS-1Y/FEB18/cdf'\n",
    "    # Define the path pattern\n",
    "    path_pattern = f'{ifs_path}/P201802*'\n",
    "    path_pattern_sfile = f'{ifs_path}/S201802*'\n",
    "\n",
    "    # Get a list of all file paths that match the pattern\n",
    "    file_paths = glob.glob(path_pattern)\n",
    "    sfile_path = glob.glob(path_pattern_sfile)\n",
    "\n",
    "    print(len(file_paths))\n",
    "    print(len(sfile_path))\n",
    "\n",
    "    from dask.distributed import Client\n",
    "    client = Client()\n",
    "\n",
    "    # Define your chunking configuration\n",
    "    chunks = {'time': 1}\n",
    "\n",
    "    # Load the first chunk of data\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "        ds_p_ifs1 = xr.open_mfdataset(file_paths[0:16], chunks=chunks, parallel=True)\n",
    "        ds_s_ifs1 = xr.open_mfdataset(sfile_path[0:16], chunks=chunks, parallel=True)\n",
    "\n",
    "    print('First 15 files loaded')\n",
    "\n",
    "    # Load the second chunk of data\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "        ds_p_ifs2 = xr.open_mfdataset(file_paths[17:], chunks=chunks, parallel=True)\n",
    "        ds_s_ifs2 = xr.open_mfdataset(sfile_path[17:], chunks=chunks, parallel=True)\n",
    "\n",
    "    print('File 17 to last one loaded files loaded')\n",
    "\n",
    "\n",
    "    # Load the single file\n",
    "    ds_single = xr.open_dataset(file_paths[16])\n",
    "    ds_s_single = xr.open_dataset(sfile_path[16])\n",
    "\n",
    "    print('File 16 loaded, now concatenating all ot them')\n",
    "\n",
    "    # Concatenate all datasets along the 'time' dimension\n",
    "    ds_p_ifs = xr.concat([ds_p_ifs1, ds_single, ds_p_ifs2], dim='time')\n",
    "    ds_s_ifs = xr.concat([ds_s_ifs1, ds_s_single, ds_s_ifs2], dim='time')\n",
    "\n",
    "    # Close the Dask client\n",
    "    client.close()\n",
    "\n",
    "    print('Finished concatenating.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   ## Select varaibles of interest in ifs_data_p\n",
    "    var_list_p = ['RWC', 'tevr', 'CC', 'T', 'OMEGA', 'Q', 'PS', 'SLP', 'LWC']   # SIWC, tsubsi and tmeltsi are latter added by combing snow adn ice immediately\n",
    "\n",
    "    ds_ifs = ds_p_ifs[var_list_p]\n",
    "\n",
    "    ## Assert that bot S and P datasts have same structure\n",
    "    def assert_coords_equal(ds1, ds2):\n",
    "        assert set(ds1.coords.keys()) == set(ds2.coords.keys()), \"Coordinate names do not match\"\n",
    "        for coord in ds1.coords:\n",
    "            assert np.all(ds1[coord] == ds2[coord]), f\"Coordinate values for {coord} do not match\"\n",
    "    assert_coords_equal(ds_ifs, ds_s_ifs)\n",
    "\n",
    "\n",
    "    var_list_s = ['RH', 'TH', 'THE', 'PV', 'VORT', 'P']\n",
    "    ds_s_ifs_selected = ds_s_ifs[var_list_s]\n",
    "\n",
    "    # Merge datasets\n",
    "    ds_ifs = ds_ifs.merge(ds_s_ifs_selected)    \n",
    "\n",
    "    print('DataSet S and P merged succesfully')\n",
    "\n",
    "    # Make field combiantion (SNOW & Ice are viewd together)\n",
    "    ds_ifs['SIWC'] = ds_p_ifs['SWC'] + ds_p_ifs['IWC']\n",
    "    ds_ifs['tsubsi'] = ds_p_ifs['tsubi'] + ds_p_ifs['tsubs']\n",
    "    ds_ifs['tmeltsi'] = ds_p_ifs['tmelti'] + ds_p_ifs['tmelts']\n",
    "    # Get rid of empty dimensions\n",
    "    ds_ifs = ds_ifs.squeeze()\n",
    "    ds_ifs = ds_ifs.sortby('time')\n",
    "\n",
    "    # Delte unnecessary datasets to free up memory\n",
    "    del ds_p_ifs\n",
    "    del ds_s_ifs\n",
    "\n",
    "\n",
    "    ## Make sure the time dimesnion of both arrays are the same\n",
    "    # cyc_mask sometimes starts at 01:00 \n",
    "    if str(ds_cycmask.time[0].values) == str(ds_ifs.time[0].values):\n",
    "        print('Start-time of ds_ifs and ds_cycmask are equal.')\n",
    "    else:\n",
    "        if ds_cycmask.time[0].values > ds_ifs.time[0].values:\n",
    "            print(f'ds_cymask starts later:\\t{str(ds_cycmask.time[0].values)}')\n",
    "            ds_ifs = ds_ifs.sel(time=slice(str(ds_cycmask.time[0].values), str(ds_ifs.time[-1].values)))\n",
    "        elif ds_cycmask.time[0].values < ds_ifs.time[0].values:\n",
    "            print(f'ds_ifs starts later:\\t{str(ds_ifs.time[0].values)}')\n",
    "            ds_cycmask = ds_cycmask.sel(time=slice(str(ds_ifs.time[0].values), str(ds_cycmask.time[-1].values)))\n",
    "        else:\n",
    "            raise ValueError('Time-horizonts of ds_cycmask and ds_ifs couldnt be resolved')\n",
    "        \n",
    "    # Make sure endtime is the same\n",
    "    if str(ds_cycmask.time[-1].values) == str(ds_ifs.time[-1].values):\n",
    "        print('End-time of ds_ifs and ds_cycmask are equal.')\n",
    "    else:\n",
    "        if ds_cycmask.time[-1].values > ds_ifs.time[-1].values:\n",
    "            print(f'ds_cymask ends later:\\t{str(ds_cycmask.time[-1].values)}')\n",
    "            ds_cycmask = ds_cycmask.sel(time=slice(str(ds_cycmask.time[0].values), str(ds_ifs.time[-1].values)))\n",
    "        elif ds_cycmask.time[-1].values < ds_ifs.time[-1].values:\n",
    "            print(f'ds_ifs ends later:\\t{str(ds_ifs.time[-1].values)}')\n",
    "            ds_ifs = ds_ifs.sel(time=slice(str(ds_ifs.time[0].values), str(ds_cycmask.time[-1].values)))\n",
    "        else:\n",
    "            raise ValueError('Time-horizonts of ds_cycmask and ds_ifs couldnt be resolved')\n",
    "\n",
    "\n",
    "    assert str(ds_cycmask.time[0].values) == str(ds_ifs.time[0].values)\n",
    "    assert str(ds_cycmask.time[-1].values) == str(ds_ifs.time[-1].values)\n",
    "    print('Data Loaded\\n')\n",
    "\n",
    "\n",
    "\n",
    "    return ds_ifs, ds_cycmask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cyclone_mask_train_validation_slpit(ds_cycmask, path):\n",
    "    ## Set a random Seed \n",
    "    random.seed(42)\n",
    "    \n",
    "    ## Get a list with all cyclone IDs\n",
    "    cyc_id_list = set(ds_cycmask.FLAG.values.flatten())\n",
    "    id_list = list(cyc_id_list)\n",
    "    ## Get number of cyclones that go into validation set\n",
    "    val_length = round(len(id_list) * 0.1)    \n",
    "    ## Randomly select 10% of entries from the list without replacement\n",
    "    val_ids = random.sample(id_list, val_length)\n",
    "    print(f'Randomlly (10%) selected validation IDs are: {val_ids}')\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------\n",
    "    ## Write information about cyclones in txt file that is store in same directroy\n",
    "    def save_txt(text, file):\n",
    "        file.write(text + '\\n')\n",
    "    # Specify the path of the output file\n",
    "    output_file_path = f'{path}/cyclone_split_info.txt'\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        save_txt(f'Number of cyclones: {len(id_list)}', file)\n",
    "        save_txt(f'ID list: {id_list}', file)\n",
    "        save_txt(f'ID of cyclones in validation set: {val_ids}', file)\n",
    "    #-----------------------------------------------------------------------------------\n",
    "    \n",
    "    print('Create cyclone mask (train and validation)')\n",
    "    ## Split trainign and validation masks based on val_ids\n",
    "    def split_train_val_cyclones(mask_field, validation_IDs):\n",
    "        mask_train, mask_val = mask_field.copy(), ds_cycmask.FLAG.copy()\n",
    "        # Create a boolean mask where True indicates the value is in val_ids\n",
    "        val_mask = xr.apply_ufunc(np.isin, mask_field, validation_IDs)\n",
    "        # For mask_train, replace all instances where val_mask is True with 0\n",
    "        mask_train = mask_train.where(~val_mask, 0)\n",
    "        # For mask_val, replace all instances where val_mask is False with 0\n",
    "        mask_val = mask_val.where(val_mask, 0)\n",
    "        return mask_train, mask_val\n",
    "    \n",
    "    FLmask_train, FLmask_val = split_train_val_cyclones(ds_cycmask['FLAG'], val_ids)\n",
    "    CFmask_train, CFmask_val = split_train_val_cyclones(ds_cycmask['CFRONTS'], val_ids)\n",
    "    WFmask_train, WFmask_val = split_train_val_cyclones(ds_cycmask['WFRONTS'], val_ids)\n",
    "    BBmask_train, BBmask_val = split_train_val_cyclones(ds_cycmask['BBFRONTS'], val_ids)\n",
    "\n",
    "    # Combine all train masks\n",
    "    cyclone_mask_train = FLmask_train + CFmask_train + WFmask_train + BBmask_train\n",
    "    cyclone_mask_train = cyclone_mask_train.where(cyclone_mask_train == 0, 1)\n",
    "    # Combine all validation masks\n",
    "    cyclone_mask_val = FLmask_val + CFmask_val + WFmask_val + BBmask_val\n",
    "    cyclone_mask_val = cyclone_mask_val.where(cyclone_mask_val == 0, 1)\n",
    "    \n",
    "    print('Save train and validation cyclone mask.\\n')\n",
    "    ## Safe the netcdf files in the data_directory under <MONTH> val_set and train_set\n",
    "    ds_cyclone_mask_train = cyclone_mask_train.to_dataset(name='cyclone_mask')\n",
    "    ds_cyclone_mask_val = cyclone_mask_val.to_dataset(name='cyclone_mask')\n",
    "    ds_cyclone_mask_train.to_netcdf(f'{path}/cyclone_mask_train.nc')\n",
    "    ds_cyclone_mask_val.to_netcdf(f'{path}/cyclone_mask_validation.nc')\n",
    "    #--------------------------------------------------------------------------------\n",
    "    return cyclone_mask_train, cyclone_mask_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def apply_mask_to_ifs(ds_ifs, mask):\n",
    "    # Create two empty dictionaries to store the 4D and 3D variables\n",
    "    data_vars_4d = {}\n",
    "    data_vars_3d = {}\n",
    "\n",
    "    # Loop over all data variables in the dataset\n",
    "    for var_name, da in ds_ifs.data_vars.items():\n",
    "        # Check if the variable has a 'lev' dimension\n",
    "        if 'lev' in da.dims:\n",
    "            # Add the variable to the 4D dictionary\n",
    "            data_vars_4d[var_name] = da\n",
    "        else:\n",
    "            # Add the variable to the 3D dictionary\n",
    "            data_vars_3d[var_name] = da\n",
    "\n",
    "    # Create datasets from the dictionaries\n",
    "    ds_4d = xr.Dataset(data_vars_4d)\n",
    "    ds_3d = xr.Dataset(data_vars_3d)\n",
    "\n",
    "    # Expand and apply the mask to the 4D dataset\n",
    "    nlevels = ds_4d.dims['lev']  # get number of levels from 4D dataset\n",
    "    mask4d = mask.expand_dims(lev=nlevels).transpose(*ds_4d.dims.keys())\n",
    "    ds_4d_masked = ds_4d.where(mask4d)\n",
    "\n",
    "    # Apply the mask to the 3D dataset\n",
    "    ds_3d_masked = ds_3d.where(mask)\n",
    "    # Combine the masked datasets back together\n",
    "    ds_ifs_masked = xr.merge([ds_4d_masked, ds_3d_masked])\n",
    "    return ds_ifs_masked\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def PRES_3d_new(data_set_PS):\n",
    "    \"\"\"\n",
    "    This function creates a 3d pressure filed based on surface pressure (model-level 0).\n",
    "    \"\"\"   \n",
    "    # Ensure aklay and bklay are Dask arrays\n",
    "    aklay = np.array([0, 0.01878906, 0.1329688, 0.4280859, 0.924414, 1.62293, 2.524805, 3.634453, 4.962383, 6.515274, 8.3075, 10.34879, 12.65398, 15.23512,    18.10488, 21.27871, 24.76691, 28.58203, 32.7325, 37.22598, 42.06668,    47.25586, 52.7909, 58.66457, 64.86477, 71.37383, 78.16859, 85.21914,    92.48985, 99.93845, 107.5174, 115.1732, 122.848, 130.4801, 138.0055,    145.3589, 152.4757, 159.2937, 165.7537, 171.8026, 177.3938, 182.4832,    187.0358, 191.0384, 194.494, 197.413, 199.8055, 201.683, 203.0566,    203.9377, 204.339, 204.2719, 203.7509, 202.7876, 201.398, 199.5966,    197.3972, 194.8178, 191.874, 188.585, 184.9708, 181.0503, 176.8462,    172.382, 167.6805, 162.7672, 157.6719, 152.4194, 147.0388, 141.5674,    136.03, 130.4577, 124.8921, 119.3581, 113.8837, 108.5065, 103.253,    98.1433, 93.19541, 88.42463, 83.83939, 79.43383, 75.1964])\n",
    "    bklay = np.array([0.9988151, 0.9963163, 0.9934933, 0.9902418, 0.9865207, 0.9823067,    0.977575, 0.9722959, 0.9664326, 0.9599506, 0.9528069, 0.944962,    0.9363701, 0.9269882, 0.9167719, 0.9056743, 0.893654, 0.8806684,    0.8666805, 0.8516564, 0.8355686, 0.8183961, 0.8001264, 0.7807572,    0.7602971, 0.7387676, 0.7162039, 0.692656, 0.6681895, 0.6428859,    0.6168419, 0.5901701, 0.5629966, 0.5354602, 0.5077097, 0.4799018,    0.4521973, 0.424758, 0.3977441, 0.3713087, 0.3455966, 0.3207688,    0.2969762, 0.274298, 0.2527429, 0.2322884, 0.212912, 0.1945903,    0.1772999, 0.1610177, 0.145719, 0.1313805, 0.1179764, 0.1054832,    0.0938737, 0.08312202, 0.07320328, 0.06408833, 0.05575071, 0.04816049,    0.04128718, 0.03510125, 0.02956981, 0.02465918, 0.02033665, 0.01656704,    0.01331083, 0.01053374, 0.008197418, 0.006255596, 0.004674384,    0.003414039, 0.002424481, 0.001672322, 0.001121252, 0.0007256266,    0.0004509675, 0.0002694785, 0.0001552459, 8.541815e-05, 4.1635e-05,   1.555435e-05, 3.39945e-06])\n",
    "    \n",
    "    interp_pres = []\n",
    "    cur_lev=0\n",
    "    for i in range(len(aklay)):\n",
    "        pres_slice = data_set_PS.PS * bklay[i] + aklay[i]\n",
    "        interp_pres.append(pres_slice)\n",
    "        cur_lev+=1\n",
    "        if cur_lev%40==0:\n",
    "            print(f'Computed {cur_lev} levels from total {len(aklay)} levels')\n",
    "\n",
    "    interp_pres = xr.concat(interp_pres, dim='lev')\n",
    "        \n",
    "    return interp_pres.transpose(*data_set_PS.dims.keys())\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def process_dataset_to_dataframe(ds, calc_rh=False):\n",
    "    \"\"\"\n",
    "    This function processes a Dataset by flattening each 4-dimensional \n",
    "    DataArray and converting the Dataset to a DataFrame. All rows with NaN \n",
    "    values are dropped.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ds : xarray.Dataset\n",
    "        The input Dataset.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        The processed DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    df_dict = {}\n",
    "    \n",
    "    # Iterate over each DataArray in the Dataset\n",
    "    for var in ds.data_vars:\n",
    "        # Check if the DataArray is 4D\n",
    "        if ds[var].ndim == 4:\n",
    "            print(f'Flatten {var}')\n",
    "            # Flatten the DataArray and add to dictionary\n",
    "            df_dict[var] = ds[var].values.flatten()\n",
    "        else:\n",
    "            print(f'Not flatten {var}, because it is a surface field')\n",
    "\n",
    "    print('Form pd.DatFrame')\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(df_dict)\n",
    "\n",
    "    shape_of_df = df.shape\n",
    "    print(f'Shape of df before dropnan: {shape_of_df[0]:4.4e}')\n",
    "    # Drop NaN values\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df, shape_of_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################  Function that combines all in one and parallelizes: #####################################################################################\n",
    "\n",
    "def get_train_validation_data_IFS18(month, calc_rh=False):\n",
    "    \n",
    "    path = f'/net/helium/atmosdyn/freimax/data_msc/IFS-18/cyclones/data_random_forest/{month}'\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(path):\n",
    "        # If not, create the directory\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    ds_ifs, ds_cycmask = load_data(month)\n",
    "\n",
    "    ## Split train and validation cyclones\n",
    "    cyc_mask_train, cyc_mask_val = cyclone_mask_train_validation_slpit(ds_cycmask, path)\n",
    "\n",
    "    print(f'Used volume of ds_ifs:\\t{(ds_ifs.nbytes / 1e9):.2f} GB\\n')\n",
    "\n",
    "\n",
    "    #=========================================== TRAIN DATA ===========================================================#\n",
    "\n",
    "    ## Apply the cyclone Mask to the IFS data\n",
    "    print('Apply cyclone mask to IFS train data')\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        ds_ifs_masked_train = apply_mask_to_ifs(ds_ifs=ds_ifs, mask=cyc_mask_train)\n",
    "\n",
    "    #-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if calc_rh:\n",
    "        # Calculate a 4d pressure filed\n",
    "        print('Get 4d pressure field for RH_ifs calcualtion')\n",
    "        pres_4d_train = PRES_3d_new(ds_ifs_masked_train)\n",
    "\n",
    "        #-------------------------------------------------------------------------\n",
    "        # Set up a Dask LocalCluster and Client\n",
    "        local_directory = tempfile.mkdtemp()\n",
    "        cluster = LocalCluster(local_directory=local_directory, n_workers=24, threads_per_worker=2, memory_limit='96GB')\n",
    "        client = Client(cluster)\n",
    "\n",
    "        print('Calculate RH_ifs for train data')\n",
    "        ds_ifs_masked_train['RH_ifs'] = calculate_rh_ifs(pres_4d_train, ds_ifs_masked_train.Q, ds_ifs_masked_train.T)\n",
    "\n",
    "        # Close the Dask Client and LocalCluster when finished\n",
    "        client.close()\n",
    "        cluster.close()\n",
    "        shutil.rmtree(local_directory) # Remove the temporary directory when done\n",
    "        #-------------------------------------------------------------------------\n",
    "\n",
    "        print('Drop varaibles (Q and PS (surface pressure)) that are superfluous')\n",
    "        ds_ifs_masked_train = ds_ifs_masked_train.drop_vars(['Q','PS'])\n",
    "\n",
    "\n",
    "    else:\n",
    "        print('RH_ifs is not calculated, but pressure at surface is dropped')\n",
    "        ds_ifs_masked_train = ds_ifs_masked_train.drop_vars(['PS'])\n",
    "    #-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # Set up a Dask LocalCluster and Client\n",
    "    local_directory = tempfile.mkdtemp()\n",
    "    cluster = LocalCluster(local_directory=local_directory, n_workers=24, threads_per_worker=2, memory_limit='96GB')\n",
    "    client = Client(cluster)\n",
    "    \n",
    "    print('Flatten dataset and convert to pd.Dataframe')\n",
    "    df_ifs_masked_train, df_train_shape_before_dropnan = process_dataset_to_dataframe(ds_ifs_masked_train)\n",
    "\n",
    "    print('Safe the dataframe with the training data')\n",
    "    df_ifs_masked_train.to_pickle(f'{path}/df_ifs_masked_train.pkl')\n",
    "\n",
    "    # Close the Dask Client and LocalCluster when finished\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    # Remove the temporary directory when done\n",
    "    shutil.rmtree(local_directory)\n",
    "\n",
    "\n",
    "    #=============================================== VALIDATION DATA ==================================================#\n",
    "\n",
    "    ## Apply the cyclone Mask to the IFS data\n",
    "    print('Apply cyclone mask to IFS validation data')\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        ds_ifs_masked_val = apply_mask_to_ifs(ds_ifs=ds_ifs, mask=cyc_mask_val)\n",
    "\n",
    "    #-------------------------------------------------------------------------\n",
    "\n",
    "    if calc_rh:\n",
    "        # Calculate a 4d pressure filed\n",
    "        print('Get 4d pressure field')\n",
    "        pres_4d_val = PRES_3d_new(ds_ifs_masked_val)\n",
    "\n",
    "        #-------------------------------------------------------------------------\n",
    "\n",
    "        print('Calculate RH_ifs for train data')\n",
    "        ds_ifs_masked_val['RH_ifs'] = calculate_rh_ifs(pres_4d_val, ds_ifs_masked_val.Q, ds_ifs_masked_val.T)\n",
    "\n",
    "        # Set up a Dask LocalCluster and Client\n",
    "        local_directory = tempfile.mkdtemp()\n",
    "        cluster = LocalCluster(local_directory=local_directory, n_workers=24, threads_per_worker=2, memory_limit='96GB')\n",
    "        client = Client(cluster)\n",
    "\n",
    "        print('Calculate RH_ifs for train data')\n",
    "        ds_ifs_masked_val['RH_ifs'] = calculate_rh_ifs(pres_4d_val, ds_ifs_masked_val.Q, ds_ifs_masked_val.T)\n",
    "        \n",
    "        # Close the Dask Client and LocalCluster when finished\n",
    "        client.close()\n",
    "        cluster.close()\n",
    "        shutil.rmtree(local_directory) # Remove the temporary directory when done\n",
    "\n",
    "        #-------------------------------------------------------------------------\n",
    "\n",
    "        print('Drop varaibles (Q and PS (surface pressure)) that are superfluous')\n",
    "        ds_ifs_masked_val = ds_ifs_masked_val.drop_vars(['Q', 'PS'])\n",
    "\n",
    "    else:\n",
    "        print('RH_ifs is not calculated, but pressure at surface is dropped')\n",
    "        ds_ifs_masked_val = ds_ifs_masked_val.drop_vars(['PS'])\n",
    "    #-------------------------------------------------------------------------\n",
    "    #-------------------------------------------------------------------------\n",
    "\n",
    "    # Set up a Dask LocalCluster and Client\n",
    "    local_directory = tempfile.mkdtemp()\n",
    "    cluster = LocalCluster(local_directory=local_directory, n_workers=24, threads_per_worker=2, memory_limit='96GB')\n",
    "    client = Client(cluster)\n",
    "    print('Flatten dataset and convert to pd.Dataframe')\n",
    "    df_ifs_masked_val, shape_before_dropnan = process_dataset_to_dataframe(ds_ifs_masked_val)\n",
    "\n",
    "    print('Safe the dataframe with the validation data')\n",
    "    df_ifs_masked_val.to_pickle(f'{path}/df_ifs_masked_val.pkl')\n",
    "\n",
    "    # Close the Dask Client and LocalCluster when finished\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    # Remove the temporary directory when done\n",
    "    shutil.rmtree(local_directory)\n",
    "\n",
    "    #-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    ## Return big dataset so it can be delted\n",
    "    #return ds_ifs_masked_val, ds_ifs\n",
    "\n",
    "###########################################################################################################################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tStart with FEB18:\n",
      "\n",
      "\n",
      "672\n",
      "672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Miniconda3-envs/envs/2022/envs/iacpy3_2022/lib/python3.9/site-packages/distributed/node.py:180: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 33239 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 15 files loaded\n",
      "File 17 to last one loaded files loaded\n",
      "File 16 loaded, now concatenating all ot them\n",
      "Finished concatenating.\n",
      "DataSet S and P merged succesfully\n",
      "Start-time of ds_ifs and ds_cycmask are equal.\n",
      "ds_cymask ends later:\t2018-03-08T00:00:00.000000000\n",
      "Data Loaded\n",
      "\n",
      "Randomlly (10%) selected validation IDs are: [73.0, 15.0]\n",
      "Create cyclone mask (train and validation)\n",
      "Save train and validation cyclone mask.\n",
      "\n",
      "Used volume of ds_ifs:\t727.97 GB\n",
      "\n",
      "Apply cyclone mask to IFS train data\n",
      "RH_ifs is not calculated, but pressure at surface is dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Miniconda3-envs/envs/2022/envs/iacpy3_2022/lib/python3.9/site-packages/distributed/node.py:180: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 36017 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten dataset and convert to pd.Dataframe\n",
      "Flatten RWC\n",
      "Flatten tevr\n",
      "Flatten CC\n",
      "Flatten T\n",
      "Flatten OMEGA\n",
      "Flatten Q\n",
      "Flatten LWC\n",
      "Flatten RH\n",
      "Flatten TH\n",
      "Flatten THE\n",
      "Flatten PV\n",
      "Flatten VORT\n",
      "Flatten P\n",
      "Flatten SIWC\n",
      "Flatten tsubsi\n",
      "Flatten tmeltsi\n",
      "Not flatten SLP, because it is a surface field\n",
      "Form pd.DatFrame\n",
      "Shape of df before dropnan: 4.6439e+08\n",
      "Safe the dataframe with the training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker process still alive after 3.9999982833862306 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999988555908206 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999990463256836 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.999997901916504 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.999998474121094 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999988555908206 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.999998664855957 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999988555908206 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999988555908206 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999988555908206 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999990463256836 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999988555908206 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999988555908206 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.999998664855957 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999982833862306 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.999998664855957 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.999998664855957 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.999998474121094 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999990463256836 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999994277954105 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.999998474121094 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999994277954105 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999988555908206 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999988555908206 seconds, killing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply cyclone mask to IFS validation data\n",
      "RH_ifs is not calculated, but pressure at surface is dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Miniconda3-envs/envs/2022/envs/iacpy3_2022/lib/python3.9/site-packages/distributed/node.py:180: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 45927 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten dataset and convert to pd.Dataframe\n",
      "Flatten RWC\n",
      "Flatten tevr\n",
      "Flatten CC\n",
      "Flatten T\n",
      "Flatten OMEGA\n",
      "Flatten Q\n",
      "Flatten LWC\n",
      "Flatten RH\n",
      "Flatten TH\n",
      "Flatten THE\n",
      "Flatten PV\n",
      "Flatten VORT\n",
      "Flatten P\n",
      "Flatten SIWC\n",
      "Flatten tsubsi\n",
      "Flatten tmeltsi\n",
      "Not flatten SLP, because it is a surface field\n",
      "Form pd.DatFrame\n",
      "Shape of df before dropnan: 4.6439e+08\n",
      "Safe the dataframe with the validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker process still alive after 3.9999990463256836 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999996185302735 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999996185302735 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.999997901916504 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999994277954105 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999996185302735 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.999999809265137 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999996185302735 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999996185302735 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999996185302735 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999994277954105 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999996185302735 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999996185302735 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999994277954105 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.999999809265137 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.999999809265137 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999996185302735 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999982833862306 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.999998664855957 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.999998664855957 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999996185302735 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999994277954105 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999996185302735 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3.9999996185302735 seconds, killing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tDONE\n",
      "\n",
      "################################################################### FINISHED WITH SCRIPT ###################################################################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gc\n",
    "if __name__ == '__main__':\n",
    "    monthlist = ['FEB18']\n",
    "\n",
    "    for month in monthlist:\n",
    "        print(f'\\n\\tStart with {month}:\\n\\n')\n",
    "        get_train_validation_data_IFS18(month, calc_rh=False)\n",
    "        print('\\n\\tDONE\\n')\n",
    "\n",
    "        # Call the garbage collector to make sure we do not run out of RAM\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    print('################################################################### FINISHED WITH SCRIPT ###################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

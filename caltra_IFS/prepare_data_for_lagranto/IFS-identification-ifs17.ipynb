{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import sys\n",
    "sys.path.append('/home/ascherrmann/scripts/')\n",
    "import helper\n",
    "import argparse\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create File with cyclone information of case study 17 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Structure of d:\n",
    "-  0 col:    Time\n",
    "-  1 col:    ID\n",
    "-  2 col:    LON\n",
    "-  3 col:    LAT\n",
    "-  4 col:    Distance\n",
    "-  5 col:    Area\n",
    "-  6 col:    Inpres\n",
    "-  7 col:    Outpres\n",
    "-  8 col:    baroclin\n",
    "-  9 col:    meanth\n",
    "- 10 col:    ctype\n",
    "- 11 col:    bergeron\n",
    "- 12 col:    bmaxdiff\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs from the TRACKED_CYCLONE file:\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datastruct = dict()\n",
    "\n",
    "labels = np.array([])\n",
    "Gdates = np.array([])\n",
    "data = dict()\n",
    "\n",
    "LON = np.round(np.linspace(-180,180,901),1)\n",
    "LAT = np.round(np.linspace(0,90,226),1)\n",
    "\n",
    "ft = date.toordinal(date(1950,1,1))\n",
    "\n",
    "add = 'IFS17-CYC-case-study'\n",
    "\n",
    "datastruct = dict()\n",
    "\n",
    "labels = np.array([])\n",
    "Gdates = np.array([])\n",
    "data = dict()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## LOAD DATA\n",
    "################################# MODIFIED BY MAX ############################################################################\n",
    "# - Load my Data and my validation IDs \n",
    "# path_trackCYC = f'/net/helium/atmosdyn/IFS-1Y/{Month}/features/tracking/TRACKED_CYCLONES'\n",
    "path_trackCYC = f'/net/helium/atmosdyn/freimax/data_msc/IFS-17/trajectories/ctraj/TRACKED_CYCLONE.txt'\n",
    "d = np.loadtxt(path_trackCYC,skiprows=1)\n",
    "IDs = np.unique(d[1])\n",
    "print(f'IDs from the TRACKED_CYCLONE file:\\n{IDs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ## Load validation cyclone IDs\n",
    "# path_CYCsplit_info = f'/net/helium/atmosdyn/freimax/data_msc/IFS-18/cyclones/data_random_forest/{MOT}/cyclone_split_info.txt'\n",
    "# with open(path_CYCsplit_info, 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "# num_cyclones = int(lines[0].split(': ')[1])  # The 'Number of cyclones' line\n",
    "# id_list_str = lines[1].split(': ')[1].strip('[]\\n')\n",
    "# id_list_cyc = [float(x) for x in id_list_str.split(', ')] if id_list_str else []  # The 'ID list' line\n",
    "# id_val_str = lines[2].split(': ')[1].strip('[]\\n')\n",
    "# IDs_valCYC = [float(x) for x in id_val_str.split(', ')] if id_val_str else []  # The 'ID of cyclones in validation set' line\n",
    "# print(f'IDs of the validation cyclones:\\n{MOT}\\t{IDs_valCYC}\\n')\n",
    "# ################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = IDs[0]\n",
    "\n",
    "## cappear contains all indices of rows in d that have ids (one of the validation IDs)        \n",
    "cappear, = np.where((d[1]==ids))\n",
    "\n",
    "\n",
    "clat = []\n",
    "clon = []\n",
    "hourszeta = np.array([])\n",
    "hourstoSLPmin = np.array([])\n",
    "zetal = np.array([])\n",
    "dates = np.array([])\n",
    "zeta = np.zeros(len(cappear))\n",
    "hours = np.zeros(len(cappear))\n",
    "\n",
    "\n",
    "hoursslp = np.array([])         # MAX\n",
    "slpl = np.array([])             # MAX\n",
    "slp = np.zeros(len(cappear))    # MAX\n",
    "\n",
    "# find id of minimum SLP in track file\n",
    "# slpminid = cappear[np.where(d[cappear,6] == np.min(d[cappear,6]))]\n",
    "# slpminid = slpminid[-1]\n",
    "# hourstoSLPmin = np.append(hourstoSLPmin, d[cappear,0]-d[slpminid,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### step through hrs of the cyclon in trackfile\n",
    "# for wr,u in enumerate(cappear):\n",
    "wr, u = 0, 0\n",
    "tmp = d\n",
    "Date = '20170410_17'\n",
    "\n",
    "# Load dataset with PS, SLP and Vorticity\n",
    "dataset_1 = xr.open_dataset(f'/net/helium/atmosdyn/freimax/data_msc/IFS-17/cdf/P{Date}')\n",
    "data[Date] = dataset_1\n",
    "\n",
    "dates = np.append(dates,Date)\n",
    "Gdates = np.append(Gdates,Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### keep adding lon to lon and lat to lat and use srtid, as it is correct with normal crosssections\n",
    "CLONIDS, CLATIDS = helper.IFS_radial_ids_correct(200,LAT[np.where(LAT==np.round(tmp[3],1))[0][0]])\n",
    "addlon = CLONIDS + np.where(LON==np.round(tmp[2],1))[0][0]\n",
    "addlon[np.where((addlon-900)>0)] = addlon[np.where((addlon-900)>0)]-900\n",
    "\n",
    "clat.append(CLATIDS.astype(int) + np.where(LAT==np.round(tmp[3],1))[0][0])\n",
    "clon.append(addlon.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-10T17:00:0\t\tMin slp: 965.0\tLonbox: [160.8, 165.6]  |  Latbox: [45.2, 48.4]\n"
     ]
    }
   ],
   "source": [
    "    slpl = np.append(slpl, data[Date].SLP.isel(lat=slice(clat[-1][0]+225,clat[-1][-1]+225) , lon=slice(clon[-1][0],clon[-1][-1])).values)\n",
    "    \n",
    "    slp[wr] = np.min(slpl)\n",
    "\n",
    "    min_slp_t = data[Date].SLP.isel(lat=slice(clat[-1][0]+225,clat[-1][-1]+225) , lon=slice(clon[-1][0],clon[-1][-1])).values.min()\n",
    "    print(f'{str(data[Date].time.values)[0:18]}\\t\\tMin slp: {min_slp_t:.1f}\\tLonbox: [{data[Date].lon[clon[-1].min()].values:.1f}, {data[Date].lon[clon[-1].max()].values:.1f}]  |  Latbox: [{data[Date].lat[clat[-1].min()+225].values:.1f}, {data[Date].lat[(clat[-1]+225).max()].values:.1f}]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.9 163.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### add 1 as clat, clon star            print(f'') with zeros as base to stack them\n",
    "MatureLat = np.round(np.mean(LAT[clat[np.where(slp==np.min(slp))[0][-1]]]),1)\n",
    "MatureLon = np.round(np.mean(LON[clon[np.where(slp==np.min(slp))[0][-1]]]),1)\n",
    "print(MatureLat, MatureLon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hours since first identification\n",
    "# hours[wr] = tmp[0]\n",
    "\n",
    "\n",
    "### use last lon and lat entries [-1] of center\n",
    "PS = data[Date].PS.values[clat[-1],clon[-1]]\n",
    "for e in range(len(CLATIDS)):\n",
    "    P = helper.modellevel_to_pressure(PS[e])\n",
    "    I = np.where(abs(P-850)==np.min(abs(P-850)))[0]\n",
    "    I = I[0].astype(int)\n",
    "    # zetal = np.append(zetal,data[Date].VORT.values[0,I,clat[-1][e],clon[-1][e]])\n",
    "\n",
    "# zeta[wr] = np.mean(zetal)\n",
    "\n",
    "\n",
    "\n",
    "# hours = hours - hours[np.where(zeta==np.max(zeta))]\n",
    "# hourszeta = np.append(hourszeta,hours)\n",
    "\n",
    "# # allow for at least 48h backward trajectories\n",
    "# if (((int(dates[np.where(zeta==np.max(zeta))[0][-1]][6:8])>=3)) | (int(dates[np.where(zeta==np.max(zeta))[0][-1]][4:6])!=MONTHN[np.where(MONTHS==MOT)[0][0]])):\n",
    "#     MatureLat = np.round(np.mean(LAT[clat[np.where(zeta==np.max(zeta))[0][-1]]]),1)\n",
    "#     MatureLon = np.round(np.mean(LON[clon[np.where(zeta==np.max(zeta))[0][-1]]]),1)\n",
    "    # use latest maximum relative vorticity\n",
    "#           #   MatureLat = np.round(np.mean(LAT[clat[np.where(zeta==np.max(zeta))[0][-1]+1]]),1)\n",
    "#           #   MatureLon = np.round(np.mean(LON[clon[np.where(zeta==np.max(zeta))[0][-1]+1]]),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.49184360e+09, 0.00000000e+00, 1.63600006e+02, 4.72000010e+01,\n",
       "       9.64964355e+02])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cyclone ID: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if((hours[0]<=(-0)) & (hours[-1]>=0)):          # MAX\n",
    "    datastruct['clat'] = clat\n",
    "    datastruct['clon'] = clon\n",
    "    datastruct['zeta'] = zeta\n",
    "    datastruct['hzeta'] = hourszeta\n",
    "    datastruct['SLP'] = d[4]\n",
    "    datastruct['hSLP'] = hourstoSLPmin\n",
    "    datastruct['dates'] = dates\n",
    "    datastruct['Matlat'] = MatureLat\n",
    "    datastruct['Matlon'] = MatureLon\n",
    "   \n",
    "    datastruct['label'] = ids\n",
    "    print('Saved cyclone ID:', ids)\n",
    "\n",
    "else: \n",
    "    print(f'File not saved, becuae time to mature stage is not long enough: {hours[0]}, {hours[-1]} ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clat': [array([113, 113, 113, 113, 113, 114, 114, 114, 114, 114, 114, 114, 114,\n",
       "         114, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 116,\n",
       "         116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 117,\n",
       "         117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 118,\n",
       "         118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 119,\n",
       "         119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 120,\n",
       "         120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 121, 121, 121,\n",
       "         121, 121, 121, 121])],\n",
       " 'clon': [array([856, 857, 858, 859, 860, 854, 855, 856, 857, 858, 859, 860, 861,\n",
       "         862, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 852,\n",
       "         853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 852,\n",
       "         853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 852,\n",
       "         853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 852,\n",
       "         853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 853,\n",
       "         854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 855, 856, 857,\n",
       "         858, 859, 860, 861])],\n",
       " 'zeta': array([0.]),\n",
       " 'hzeta': array([], dtype=float64),\n",
       " 'SLP': 964.964355,\n",
       " 'hSLP': array([], dtype=float64),\n",
       " 'dates': array(['20170410_17'], dtype='<U32'),\n",
       " 'Matlat': 46.9,\n",
       " 'Matlon': 163.2,\n",
       " 'label': 0.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/net/helium/atmosdyn/freimax/data_msc/IFS-17'\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "f = open(PATH + '/' + add + '.txt','wb')\n",
    "pickle.dump(datastruct,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IFS17-CYC-case-study'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maxpy3_2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

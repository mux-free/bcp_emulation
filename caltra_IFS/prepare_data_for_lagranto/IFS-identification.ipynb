{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import sys\n",
    "sys.path.append('/home/ascherrmann/scripts/')\n",
    "import helper\n",
    "import argparse\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create File with cyclone information of case study 17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/freimax/msc_thesis/scripts/caltra_IFS/prepare_data_for_lagranto/IFS-identification.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biacdipl-6.ethz.ch/home/freimax/msc_thesis/scripts/caltra_IFS/prepare_data_for_lagranto/IFS-identification.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m path_trackCYC \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/net/helium/atmosdyn/freimax/data_msc/IFS-17/trajectories/caltra/TRACKED_CYCLONE.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biacdipl-6.ethz.ch/home/freimax/msc_thesis/scripts/caltra_IFS/prepare_data_for_lagranto/IFS-identification.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m d \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mloadtxt(path_trackCYC,skiprows\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Biacdipl-6.ethz.ch/home/freimax/msc_thesis/scripts/caltra_IFS/prepare_data_for_lagranto/IFS-identification.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m IDs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(d[:,\u001b[39m1\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biacdipl-6.ethz.ch/home/freimax/msc_thesis/scripts/caltra_IFS/prepare_data_for_lagranto/IFS-identification.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mIDs from the TRACKED_CYCLONE file:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mIDs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "MOT = \"APR17\"\n",
    "\n",
    "path_trackCYC = f'/net/helium/atmosdyn/freimax/data_msc/IFS-17/trajectories/caltra/TRACKED_CYCLONE.txt'\n",
    "d = np.loadtxt(path_trackCYC,skiprows=1)\n",
    "try:\n",
    "    IDs = np.unique(d[:,1])\n",
    "except IndexError:\n",
    "    IDs = np.unique(d[-1])\n",
    "print(f'IDs from the TRACKED_CYCLONE file:\\n{IDs}')\n",
    "\n",
    "\n",
    "# ## Load validation cyclone IDs\n",
    "# path_CYCsplit_info = f'/net/helium/atmosdyn/freimax/data_msc/IFS-18/cyclones/data_random_forest/{MOT}/cyclone_split_info.txt'\n",
    "# with open(path_CYCsplit_info, 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "\n",
    "# num_cyclones = int(lines[0].split(': ')[1])  # The 'Number of cyclones' line\n",
    "# id_list_str = lines[1].split(': ')[1].strip('[]\\n')\n",
    "# id_list_cyc = [float(x) for x in id_list_str.split(', ')] if id_list_str else []  # The 'ID list' line\n",
    "# id_val_str = lines[2].split(': ')[1].strip('[]\\n')\n",
    "# IDs_valCYC = [float(x) for x in id_val_str.split(', ')] if id_val_str else []  # The 'ID of cyclones in validation set' line\n",
    "# print('IDs of the validation cyclones:\\n',MOT, '\\t',IDs_valCYC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccc = 0\n",
    "datesss=[]\n",
    "for ids in [IDs_valCYC[-1]]:          # MAX\n",
    "    ## cappear contains all indices of rows in d that have ids (one of the validation IDs)        \n",
    "    cappear, = np.where((d[:,1]==ids))\n",
    "    for wr,u in enumerate(cappear):\n",
    "        tmp = d[u]\n",
    "        k = str(helper.datenum_to_datetime(tmp[0]/24))\n",
    "        Date = k[0:4]+k[5:7]+k[8:10]+'_'+k[11:13]\n",
    "        ccc +=1\n",
    "        datesss.append(Date)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAT = np.round(np.linspace(0,90,226),1)\n",
    "LON = np.round(np.linspace(-180,180,901),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAT[np.where(LAT==np.round(tmp[3],1))[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([54.4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAT[np.where(LAT==np.round(tmp[3],1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(LON==np.round(tmp[2],1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addlon = CLONIDS + np.where(LON==np.round(tmp[2],1))[0][0]\n",
    "addlon[np.where((addlon-900)>0)] = addlon[np.where((addlon-900)>0)]-900\n",
    "\n",
    "addlon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Structure of d:\n",
    "-  0 col:    Time\n",
    "-  1 col:    ID\n",
    "-  2 col:    LON\n",
    "-  3 col:    LAT\n",
    "-  4 col:    Distance\n",
    "-  5 col:    Area\n",
    "-  6 col:    Inpres\n",
    "-  7 col:    Outpres\n",
    "-  8 col:    baroclin\n",
    "-  9 col:    meanth\n",
    "- 10 col:    ctype\n",
    "- 11 col:    bergeron\n",
    "- 12 col:    bmaxdiff\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "START IDENTIFICATION OF MONTH: DEC17\n",
      "------------------------------\n",
      "\n",
      "IDs from the TRACKED_CYCLONE file:\n",
      "[ 16.  30.  33.  38.  42.  52.  53.  55.  66.  69.  70.  74.  75.  77.\n",
      "  78.  79.  85. 102. 103. 111. 116. 118. 119. 123.]\n",
      "IDs of the validation cyclones:\n",
      "DEC17\t[111.0, 33.0]\n",
      "\n",
      "Compute on CYC_id:  111.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53770/1818112968.py:128: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if (~(np.any(Gdates==Date))):                           # ALEX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cyclone ID: 111.0\n",
      "Compute on CYC_id:  33.0\n",
      "Saved cyclone ID: 33.0\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------\n",
    "## I think this is stuff ALex only needed for mediteranean cyclones\n",
    "# LATM = np.array([30,42,30,48])\n",
    "# LONM = np.array([-5,2,42])\n",
    "\n",
    "# #LONB = np.array([-105,-75,40,90])\n",
    "# LONB = np.array([-95,-75,-5,15])\n",
    "# LATB = np.array([15,83,0,83,50,83])\n",
    "\n",
    "# LATSS = [LATM, LATB]\n",
    "# LONSS = [LONM,LONB]\n",
    "#---------------------------------------------------------------------\n",
    "#r200_lonids, r200_latids = helper.radial_ids_around_center(200)\n",
    "\n",
    "datastruct = dict()\n",
    "#llat = len(r200_lonids)\n",
    "\n",
    "labels = np.array([])\n",
    "Gdates = np.array([])\n",
    "data = dict()\n",
    "###  restrict myself to Atlantic, tropical and Mediterranean ones\n",
    "\n",
    "\n",
    "\n",
    "MONTHS = np.array(['DEC17','JAN18','FEB18','MAR18','APR18','MAY18','JUN18','JUL18','AUG18','SEP18','OCT18','NOV18'])\n",
    "MONTHN = np.append(12,np.arange(1,12))\n",
    "\n",
    "\n",
    "# # for at least 48 h backward trajectories require mature stage to be after the following dates\n",
    "# mx = 3\n",
    "\n",
    "MOT = 'DEC17'\n",
    "\n",
    "months=[MOT]\n",
    "\n",
    "LON = np.round(np.linspace(-180,180,901),1)\n",
    "LAT = np.round(np.linspace(0,90,226),1)\n",
    "\n",
    "ft = date.toordinal(date(1950,1,1))\n",
    "\n",
    "add = 'All-CYC-entire-year-maxf' + MOT + '-correct'\n",
    "\n",
    "datastruct = dict()\n",
    "\n",
    "labels = np.array([])\n",
    "Gdates = np.array([])\n",
    "data = dict()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###  restrict myself to Atlantic, tropical and Mediterranean ones\n",
    "for Month in months:\n",
    "    datastruct[Month] = dict()\n",
    "              \n",
    "    print(f'\\nSTART IDENTIFICATION OF MONTH: {Month}\\n------------------------------\\n')\n",
    "\n",
    "\n",
    "    ## LOAD DATA\n",
    "    ################################# MODIFIED BY MAX ############################################################################\n",
    "    # - Load my Data and my validation IDs \n",
    "    path_trackCYC = f'/net/helium/atmosdyn/IFS-1Y/{Month}/features/tracking/TRACKED_CYCLONES'\n",
    "    d = np.loadtxt(path_trackCYC,skiprows=1)\n",
    "    IDs = np.unique(d[:,1])\n",
    "    print(f'IDs from the TRACKED_CYCLONE file:\\n{IDs}')\n",
    "\n",
    "    ## Load validation cyclone IDs\n",
    "    path_CYCsplit_info = f'/net/helium/atmosdyn/freimax/data_msc/IFS-18/cyclones/data_random_forest/{MOT}/cyclone_split_info.txt'\n",
    "    with open(path_CYCsplit_info, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    num_cyclones = int(lines[0].split(': ')[1])  # The 'Number of cyclones' line\n",
    "    id_list_str = lines[1].split(': ')[1].strip('[]\\n')\n",
    "    id_list_cyc = [float(x) for x in id_list_str.split(', ')] if id_list_str else []  # The 'ID list' line\n",
    "    id_val_str = lines[2].split(': ')[1].strip('[]\\n')\n",
    "    IDs_valCYC = [float(x) for x in id_val_str.split(', ')] if id_val_str else []  # The 'ID of cyclones in validation set' line\n",
    "    print(f'IDs of the validation cyclones:\\n{MOT}\\t{IDs_valCYC}\\n')\n",
    "    ################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    # for ids in IDs[:]:            # ALEX\n",
    "    for ids in IDs_valCYC:          # MAX\n",
    "\n",
    "        print('Compute on CYC_id: ', ids)\n",
    "\n",
    "        ## cappear contains all indices of rows in d that have ids (one of the validation IDs)        \n",
    "        cappear, = np.where((d[:,1]==ids))\n",
    "\n",
    "\n",
    "        clat = []\n",
    "        clon = []\n",
    "        hourszeta = np.array([])\n",
    "        hourstoSLPmin = np.array([])\n",
    "        zetal = np.array([])\n",
    "        dates = np.array([])\n",
    "        zeta = np.zeros(len(cappear))\n",
    "        hours = np.zeros(len(cappear))\n",
    "        \n",
    "        \n",
    "        hoursslp = np.array([])         # MAX\n",
    "        slpl = np.array([])             # MAX\n",
    "        slp = np.zeros(len(cappear))    # MAX\n",
    "\n",
    "\n",
    "        ### find id of minimum SLP in track file\n",
    "        slpminid = cappear[np.where(d[cappear,6] == np.min(d[cappear,6]))]\n",
    "        slpminid = slpminid[-1]\n",
    "        hourstoSLPmin = np.append(hourstoSLPmin, d[cappear,0]-d[slpminid,0])\n",
    "\n",
    "\n",
    "        ## FROM MAX: Here the cappear file column 4 (5th column) is not correct\n",
    "        # ### find id of minimum SLP in track file\n",
    "        # slpminid = cappear[np.where(d[cappear,4] == np.min(d[cappear,4]))]      # NOTE: d[cappear,4] IS EQUAL TO d[:,4]   --> values of 5th column = \"inpres\"\n",
    "        # slpminid = slpminid[-1]                                                 # Extract number from array\n",
    "        # hourstoSLPmin = np.append(hourstoSLPmin, d[cappear,0]-d[slpminid,0])    # Substract time from min SLP from other times to get seconds to/from SLP minimum\n",
    "\n",
    "\n",
    "\n",
    "        ### step through hrs of the cyclon in trackfile\n",
    "        for wr,u in enumerate(cappear):\n",
    "            tmp = d[u]\n",
    "            k = str(helper.datenum_to_datetime(ft+tmp[0]/24))\n",
    "            Date = k[0:4]+k[5:7]+k[8:10]+'_'+k[11:13]\n",
    "\n",
    "\n",
    "            ### if that date has not been loaded yet, load it\n",
    "            # if Gdates.size == 0 or (~(np.any(Gdates==Date))):     # MAX\n",
    "            if (~(np.any(Gdates==Date))):                           # ALEX\n",
    "                # Load dataset with PS, SLP and Vorticity\n",
    "                dataset_1 = xr.open_dataset(f'/net/helium/atmosdyn/IFS-1Y/{Month}/cdf/S{Date}', drop_variables=['PV','P','TH','THE','RH','PVRCONVT','PVRCONVM','PVRTURBT','PVRTURBM','PVRLS','PVRCOND','PVRSW','PVRLWH','PVRLWC','PVRDEP','PVREVC','PVREVR','PVRSUBI','PVRSUBS','PVRMELTI','PVRMELTS','PVRFRZ','PVRRIME','PVRBF'])\n",
    "                data[Date] = dataset_1\n",
    "            else:\n",
    "                 print('\\nIMPRTANT SECTION WAS LEFT OUT\\n')\n",
    "            \n",
    "            dates = np.append(dates,Date)\n",
    "            Gdates = np.append(Gdates,Date)\n",
    "                \n",
    "                ####################################################\n",
    "\n",
    "\n",
    "\n",
    "            #hours since first identification\n",
    "            hours[wr] = tmp[0]\n",
    "            ### keep adding lon to lon and lat to lat and use srtid, as it is correct with normal crosssections\n",
    "            CLONIDS, CLATIDS = helper.IFS_radial_ids_correct(200,LAT[np.where(LAT==np.round(tmp[3],1))[0][0]])\n",
    "            addlon = CLONIDS + np.where(LON==np.round(tmp[2],1))[0][0]\n",
    "            addlon[np.where((addlon-900)>0)] = addlon[np.where((addlon-900)>0)]-900\n",
    "\n",
    "            clat.append(CLATIDS.astype(int) + np.where(LAT==np.round(tmp[3],1))[0][0])\n",
    "            clon.append(addlon.astype(int))\n",
    "#                 clat = np.vstack((clat, CLATIDS + np.where(LAT==np.round(tmp[3],1))[0][0]))\n",
    "#                 clon = np.vstack((clon, addlon))\n",
    "\n",
    "            ### use last lon and lat entries [-1] of center\n",
    "            PS = data[Date].PS.values[0,0,clat[-1],clon[-1]]\n",
    "            for e in range(len(CLATIDS)):\n",
    "                P = helper.modellevel_to_pressure(PS[e])\n",
    "                I = np.where(abs(P-850)==np.min(abs(P-850)))[0]\n",
    "                I = I[0].astype(int)\n",
    "                zetal = np.append(zetal,data[Date].VORT.values[0,I,clat[-1][e],clon[-1][e]])\n",
    "\n",
    "            zeta[wr] = np.mean(zetal)\n",
    "\n",
    "        hours = hours - hours[np.where(zeta==np.max(zeta))]\n",
    "        hourszeta = np.append(hourszeta,hours)\n",
    "\n",
    "        # allow for at least 48h backward trajectories\n",
    "        if (((int(dates[np.where(zeta==np.max(zeta))[0][-1]][6:8])>=3)) | (int(dates[np.where(zeta==np.max(zeta))[0][-1]][4:6])!=MONTHN[np.where(MONTHS==MOT)[0][0]])):\n",
    "\n",
    "            ### added 30.10.2020 to ensure only MED cyclones\n",
    "            ### add 1 as clat, clon start with zeros as base to stack them\n",
    "\n",
    "            MatureLat = np.round(np.mean(LAT[clat[np.where(zeta==np.max(zeta))[0][-1]]]),1)\n",
    "            MatureLon = np.round(np.mean(LON[clon[np.where(zeta==np.max(zeta))[0][-1]]]),1)\n",
    "            # use latest maximum relative vorticity\n",
    "    #              MatureLat = np.round(np.mean(LAT[clat[np.where(zeta==np.max(zeta))[0][-1]+1]]),1)\n",
    "    #              MatureLon = np.round(np.mean(LON[clon[np.where(zeta==np.max(zeta))[0][-1]+1]]),1)\n",
    "        \n",
    "        #if time to mature stage is too short remove from data\n",
    "        # if((hours[0]<=(-2)) & (hours[-1]>=2)):        # ALEX\n",
    "        if((hours[0]<=(-0)) & (hours[-1]>=0)):          # MAX\n",
    "            datastruct[Month][ids] = dict()\n",
    "            datastruct[Month][ids]['clat'] = clat\n",
    "            datastruct[Month][ids]['clon'] = clon\n",
    "            datastruct[Month][ids]['zeta'] = zeta\n",
    "            datastruct[Month][ids]['hzeta'] = hourszeta\n",
    "            datastruct[Month][ids]['SLP'] = d[cappear,6]\n",
    "            datastruct[Month][ids]['hSLP'] = hourstoSLPmin\n",
    "            datastruct[Month][ids]['dates'] = dates\n",
    "            datastruct[Month][ids]['Matlat'] = MatureLat\n",
    "            datastruct[Month][ids]['Matlon'] = MatureLon\n",
    "            # ### these are preidentified tropical\n",
    "            # if(d[cappear[0],-3]==0):\n",
    "            #         label=0\n",
    "            # ### MED\n",
    "            # else:\n",
    "            #     for b,o in enumerate(LONM[:-1]):\n",
    "            #         if( ((30-MatureLat)<=0) & ((LATM[b]-MatureLat)>=0) & ((o-MatureLon)<=0) & ((LONM[b+1]-MatureLon)>=0)):\n",
    "            #                 label=2\n",
    "            # ### ATL\n",
    "            #         else:\n",
    "            #             label=1\n",
    "            datastruct[Month][ids]['label'] = ids\n",
    "            print('Saved cyclone ID:', ids)\n",
    "\n",
    "        else: \n",
    "            print(f'File not saved, becuae time to mature stage is not long enough: {hours[0]}, {hours[-1]} ')\n",
    "\n",
    "PATH = '/net/helium/atmosdyn/freimax/data_msc/IFS-18/IFS-traj/CYC_validation'\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(PATH + '/' + MOT, exist_ok=True)\n",
    "\n",
    "f = open(PATH + '/' + MOT + '/' + add + '.txt','wb')\n",
    "pickle.dump(datastruct,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20171212_02'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40608/561977917.py:2: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if Gdates1.size != 0 or (~(np.any(Gdates1==Date))):   # MAX\n"
     ]
    }
   ],
   "source": [
    "Gdates1 = np.array([])\n",
    "if Gdates1.size != 0 or (~(np.any(Gdates1==Date))):   # MAX\n",
    "    print('jup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Max tries stuff here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "track = '/net/helium/atmosdyn/freimax/data_msc/casestudy_ra/data/TRACKED_CYCLONE.txt'\n",
    "d = np.loadtxt(track, skiprows=1)\n",
    "\n",
    "\n",
    "# d is a list:            time  |  ?  |  lon  |  lat  | SLP-min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.49178600e+09, 0.00000000e+00, 1.58000000e+02, 4.55999980e+01,\n",
       "        9.84758118e+02],\n",
       "       [1.49178960e+09, 0.00000000e+00, 1.58399994e+02, 4.55999980e+01,\n",
       "        9.82932495e+02],\n",
       "       [1.49179320e+09, 0.00000000e+00, 1.58399994e+02, 4.60000000e+01,\n",
       "        9.81590576e+02],\n",
       "       [1.49179680e+09, 0.00000000e+00, 1.58800003e+02, 4.60000000e+01,\n",
       "        9.79677490e+02]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[:,1]\n",
    "d[1:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cappear, = np.where((d[:,1]==d[:,1][0]))\n",
    "cappear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slpminid = cappear[np.where(d[cappear,4] == np.min(d[cappear,4]))]      # NOTE: d[cappear,4] IS EQUAL TO d[:,4]   --> values of 5th column = \"inpres\"\n",
    "slpminid = slpminid[-1]\n",
    "slpminid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-61200., -57600., -54000., -50400., -46800., -43200., -39600.,\n",
       "       -36000., -32400., -28800., -25200., -21600., -18000., -14400.,\n",
       "       -10800.,  -7200.,  -3600.,      0.,   3600.,   7200.,  10800.,\n",
       "        14400.,  18000.,  21600.,  25200.,  28800.,  32400.,  36000.,\n",
       "        39600.,  43200.,  46800.,  50400.,  54000.,  57600.,  61200.,\n",
       "        64800.,  68400.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourstoSLPmin = np.array([])\n",
    "hourstoSLPmin = np.append(hourstoSLPmin, d[cappear,0]-d[slpminid,0])\n",
    "\n",
    "hourstoSLPmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "Case study cyclone APR17 added (Max)\n",
      "['2017-04-10T00:00\t\tMin slp: 985.8\tLonbox: [155.2, 160.0]  |  Latbox: [43.2, 46.4]\n",
      "['2017-04-10T01:00\t\tMin slp: 984.8\tLonbox: [155.2, 160.0]  |  Latbox: [43.6, 46.8]\n",
      "['2017-04-10T02:00\t\tMin slp: 982.9\tLonbox: [155.6, 160.4]  |  Latbox: [43.6, 46.8]\n",
      "['2017-04-10T03:00\t\tMin slp: 981.6\tLonbox: [155.6, 160.4]  |  Latbox: [44.0, 47.2]\n",
      "['2017-04-10T04:00\t\tMin slp: 979.7\tLonbox: [156.0, 160.8]  |  Latbox: [44.0, 47.2]\n",
      "['2017-04-10T05:00\t\tMin slp: 978.1\tLonbox: [156.0, 160.8]  |  Latbox: [44.0, 47.2]\n",
      "['2017-04-10T06:00\t\tMin slp: 976.4\tLonbox: [156.4, 161.2]  |  Latbox: [44.4, 47.6]\n",
      "['2017-04-10T07:00\t\tMin slp: 974.7\tLonbox: [156.4, 161.2]  |  Latbox: [44.4, 47.6]\n",
      "['2017-04-10T08:00\t\tMin slp: 973.4\tLonbox: [156.8, 161.6]  |  Latbox: [44.4, 47.6]\n",
      "['2017-04-10T09:00\t\tMin slp: 972.2\tLonbox: [156.8, 161.6]  |  Latbox: [44.4, 47.6]\n",
      "['2017-04-10T10:00\t\tMin slp: 971.3\tLonbox: [157.2, 162.0]  |  Latbox: [44.4, 47.6]\n",
      "['2017-04-10T11:00\t\tMin slp: 970.7\tLonbox: [158.0, 162.8]  |  Latbox: [44.4, 47.6]\n",
      "['2017-04-10T12:00\t\tMin slp: 969.6\tLonbox: [158.4, 163.2]  |  Latbox: [44.4, 47.6]\n",
      "['2017-04-10T13:00\t\tMin slp: 968.5\tLonbox: [158.8, 163.6]  |  Latbox: [44.8, 48.0]\n",
      "['2017-04-10T14:00\t\tMin slp: 967.0\tLonbox: [159.6, 164.4]  |  Latbox: [44.8, 48.0]\n",
      "['2017-04-10T15:00\t\tMin slp: 966.1\tLonbox: [160.0, 164.8]  |  Latbox: [44.8, 48.0]\n",
      "['2017-04-10T16:00\t\tMin slp: 965.3\tLonbox: [160.4, 165.2]  |  Latbox: [45.2, 48.4]\n",
      "['2017-04-10T17:00\t\tMin slp: 965.0\tLonbox: [160.8, 165.6]  |  Latbox: [45.2, 48.4]\n",
      "['2017-04-10T18:00\t\tMin slp: 965.1\tLonbox: [160.8, 165.6]  |  Latbox: [45.2, 48.4]\n",
      "['2017-04-10T19:00\t\tMin slp: 965.4\tLonbox: [161.2, 166.0]  |  Latbox: [45.2, 48.4]\n",
      "['2017-04-10T20:00\t\tMin slp: 965.8\tLonbox: [162.0, 166.8]  |  Latbox: [45.2, 48.4]\n",
      "['2017-04-10T21:00\t\tMin slp: 966.1\tLonbox: [162.4, 167.2]  |  Latbox: [45.2, 48.4]\n",
      "['2017-04-10T22:00\t\tMin slp: 966.2\tLonbox: [163.2, 168.0]  |  Latbox: [45.2, 48.4]\n",
      "['2017-04-10T23:00\t\tMin slp: 966.2\tLonbox: [163.6, 168.4]  |  Latbox: [45.2, 48.4]\n",
      "['2017-04-11T00:00\t\tMin slp: 966.4\tLonbox: [164.4, 169.2]  |  Latbox: [45.2, 48.4]\n",
      "['2017-04-11T01:00\t\tMin slp: 966.5\tLonbox: [164.8, 169.6]  |  Latbox: [45.6, 48.8]\n",
      "['2017-04-11T02:00\t\tMin slp: 966.4\tLonbox: [165.6, 170.4]  |  Latbox: [45.6, 48.8]\n",
      "['2017-04-11T03:00\t\tMin slp: 966.6\tLonbox: [166.0, 170.8]  |  Latbox: [46.0, 49.2]\n",
      "['2017-04-11T04:00\t\tMin slp: 966.8\tLonbox: [166.4, 171.2]  |  Latbox: [46.0, 49.2]\n",
      "['2017-04-11T05:00\t\tMin slp: 967.5\tLonbox: [167.2, 172.0]  |  Latbox: [46.4, 49.6]\n",
      "['2017-04-11T06:00\t\tMin slp: 968.0\tLonbox: [167.6, 172.4]  |  Latbox: [46.4, 49.6]\n",
      "['2017-04-11T07:00\t\tMin slp: 968.7\tLonbox: [168.0, 172.8]  |  Latbox: [46.4, 49.6]\n",
      "['2017-04-11T08:00\t\tMin slp: 969.4\tLonbox: [168.4, 173.2]  |  Latbox: [46.8, 50.0]\n",
      "['2017-04-11T09:00\t\tMin slp: 970.1\tLonbox: [168.8, 173.6]  |  Latbox: [46.8, 50.0]\n",
      "['2017-04-11T10:00\t\tMin slp: 970.5\tLonbox: [169.2, 174.0]  |  Latbox: [47.2, 50.4]\n",
      "['2017-04-11T11:00\t\tMin slp: 970.9\tLonbox: [169.6, 174.4]  |  Latbox: [47.2, 50.4]\n",
      "['2017-04-11T12:00\t\tMin slp: 970.9\tLonbox: [170.0, 174.8]  |  Latbox: [47.6, 50.8]\n",
      "49.2 172.4\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"produce txt files with identification:\")\n",
    "# parser.add_argument('months',default='',type=str,help='month combination like DJF')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# MOT = str(args.months)\n",
    "\n",
    "\n",
    "\n",
    "############### ADDED BY MAX #######################\n",
    "MOT = 'APR17'       # WE CANNOT USE ARGPARSE I IPYNB\n",
    "####################################################\n",
    "\n",
    "\n",
    "MONTHS = np.array(['DEC17','JAN18','FEB18','MAR17','APR17','MAY18','JUN18','JUL18','AUG18','SEP18','OCT18','NOV18'])\n",
    "MONTHN = np.append(12,np.arange(1,12))\n",
    "\n",
    "months=[MOT]\n",
    "\n",
    "LON = np.round(np.linspace(-180,180,901),1)\n",
    "LAT = np.round(np.linspace(0,90,226),1)\n",
    "\n",
    "\n",
    "ft = date.toordinal(date(1950,1,1))\n",
    "\n",
    "add = 'CYC-CaseStudy' + MOT + '-correct'\n",
    "#r200_lonids, r200_latids = helper.radial_ids_around_center(200)\n",
    "\n",
    "datastruct = dict()\n",
    "#llat = len(r200_lonids)\n",
    "\n",
    "labels = np.array([])\n",
    "Gdates = np.array([])\n",
    "data = dict()\n",
    "\n",
    "IDs = np.unique(d[:,1])\n",
    "for ids in IDs[:]:\n",
    "    print(type(ids))\n",
    "    IDs = ids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for Month in months:\n",
    "    datastruct[Month] = dict()\n",
    "\n",
    "    ## LOAD DATA\n",
    "    ### added by MAX-----------------------------------------------------------------------------------\n",
    "    print(f'Case study cyclone {Month} added (Max)')\n",
    "    track = '/net/helium/atmosdyn/freimax/data_msc/casestudy_ra/data/TRACKED_CYCLONE.txt'\n",
    "    d = np.loadtxt(track, skiprows=1)\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # Return an array with all indices that satisfy d[:,1]==d[:,1][0]  -> Return an array with ALL indices (from 0 to 36)\n",
    "    cappear, = np.where((d[:,1]==d[:,1][0]))\n",
    "    \n",
    "    clat = []\n",
    "    clon = []\n",
    "    hourstoSLPmin = np.array([])\n",
    "    dates = np.array([])\n",
    "    hours = np.zeros(len(cappear))\n",
    "    \n",
    "    # hourszeta = np.array([])\n",
    "    # zetal = np.array([])\n",
    "    # zeta = np.zeros(len(cappear))\n",
    "    \n",
    "    hoursslp = np.array([])\n",
    "    slpl = np.array([])\n",
    "    slp = np.zeros(len(cappear))\n",
    "\n",
    "\n",
    "    ### find id of minimum SLP in track file\n",
    "    slpminid = cappear[np.where(d[cappear,4] == np.min(d[cappear,4]))]      # NOTE: d[cappear,4] IS EQUAL TO d[:,4]   --> values of 5th column = \"inpres\"\n",
    "    slpminid = slpminid[-1]                                                 # Extract number from array\n",
    "    \n",
    "    hourstoSLPmin = np.append(hourstoSLPmin, d[cappear,0]-d[slpminid,0])    # Substract time from min SLP from other times to get seconds to/from SLP minimum\n",
    "\n",
    "\n",
    "    ### step through hrs of the cyclon in trackfile\n",
    "    for wr,u in enumerate(cappear):\n",
    "\n",
    "        tmp = d[u]\n",
    "        k = str(datetime.utcfromtimestamp(tmp[0]))\n",
    "        Date = k[0:4]+k[5:7]+k[8:10]+'_'+k[11:13]\n",
    "\n",
    "\n",
    "        ### if that date has not been loaded yet, load it\n",
    "        if Gdates.size == 0 or (~(np.any(Gdates==Date))):\n",
    "\n",
    "            #### Date generation from datetime floats (Max) ####\n",
    "            # Only retain Q, RWC, LWC, IWC, SWC, CC, T, OMEGA, U, V, SLP, PS, tsubi, tevr, tsubs, tmelti, tmelts\n",
    "            dataset_1 = xr.open_dataset('/net/thermo/atmosdyn2/atroman/PACaccu/cdf/P'+Date, drop_variables=['ttot','tdyn','tsw','tlw','tmix','tconv','tls','tcond','tdep','tbf','tevc','tfrz','trime','tce'])\n",
    "            data[Date] = dataset_1\n",
    "            dates = np.append(dates,Date)\n",
    "            Gdates = np.append(Gdates,Date)\n",
    "            ####################################################\n",
    "\n",
    "\n",
    "            # seconds since first identification\n",
    "            hours[wr] = tmp[0]      # hours was initalized as np.zeros(len(cappear))\n",
    "\n",
    "\n",
    "            ######################################## RADIUS AROUND SLP-MIN DO NOT UDNERSTADN HOW YET #################################\n",
    "            ### keep adding lon to lon and lat to lat and use srtid, as it is correct with normal crosssections\n",
    "            CLONIDS, CLATIDS = helper.IFS_radial_ids_correct(200, LAT[np.where(LAT==np.round(tmp[3],1))[0][0]] )\n",
    "            addlon = CLONIDS + np.where(LON==np.round(tmp[2],1))[0][0]\n",
    "            addlon[np.where((addlon-900)>0)] = addlon[np.where((addlon-900)>0)]-900\n",
    "            clat.append(CLATIDS.astype(int) + np.where(LAT==np.round(tmp[3],1))[0][0])\n",
    "            clon.append(addlon.astype(int))\n",
    "            #------------------------------------------------------------------------------------------------------------------------\n",
    "            \n",
    "\n",
    "                # for e in range(len(CLATIDS)):\n",
    "                #     P = helper.modellevel_to_pressure(PS[e])\n",
    "                #     I = np.where(abs(P-850)==np.min(abs(P-850)))[0]\n",
    "                #     I = I[0].astype(int)\n",
    "                #     zetal = np.append(zetal,data[Date].VORT.values[0,I,clat[-1][e],clon[-1][e]])\n",
    "\n",
    "\n",
    "    \n",
    "            slpl = np.append(slpl, data[Date].SLP.isel(lat=slice(clat[-1][0]+225,clat[-1][-1]+225) , lon=slice(clon[-1][0],clon[-1][-1])).values)\n",
    "            \n",
    "            slp[wr] = np.min(slpl)\n",
    "\n",
    "            min_slp_t = data[Date].SLP.isel(lat=slice(clat[-1][0]+225,clat[-1][-1]+225) , lon=slice(clon[-1][0],clon[-1][-1])).values.min()\n",
    "            print(f'{str(data[Date].time.values)[0:18]}\\t\\tMin slp: {min_slp_t:.1f}\\tLonbox: [{data[Date].lon[clon[-1].min()].values:.1f}, {data[Date].lon[clon[-1].max()].values:.1f}]  |  Latbox: [{data[Date].lat[clat[-1].min()+225].values:.1f}, {data[Date].lat[(clat[-1]+225).max()].values:.1f}]')\n",
    "\n",
    "\n",
    "\n",
    "    # hours = hours - hours[np.where(slp==np.min(slp))]\n",
    "    # hoursslp = np.append(hoursslp,hours)\n",
    "    \n",
    "    \n",
    "    # if  ( (int( dates[np.where(zeta==np.max(zeta))[0][-1]][6:8] ) >=3 ) ) | (int(dates[np.where(zeta==np.max(zeta))[0][-1]][4:6])!=MONTHN[np.where(MONTHS==MOT)[0][0]] ):\n",
    "        ### add 1 as clat, clon start with zeros as base to stack them\n",
    "        # MatureLat = np.round(np.mean(LAT[clat[np.where(zeta==np.max(zeta))[0][-1]]]),1)\n",
    "        # MatureLon = np.round(np.mean(LON[clon[np.where(zeta==np.max(zeta))[0][-1]]]),1)\n",
    "\n",
    "    if  ( (int( dates[np.where(slp==np.min(slp))[0][-1]][6:8] ) >=3 ) ) | (int(dates[np.where(slp==np.min(slp))[0][-1]][4:6])!=MONTHN[np.where(MONTHS==MOT)[0][0]] ):        # THIS CONDTION MAKES NO SENSE FOR MEEE?????!!!!!\n",
    "\n",
    "        ### add 1 as clat, clon star            print(f'') with zeros as base to stack them\n",
    "        MatureLat = np.round(np.mean(LAT[clat[np.where(slp==np.min(slp))[0][-1]]]),1)\n",
    "        MatureLon = np.round(np.mean(LON[clon[np.where(slp==np.min(slp))[0][-1]]]),1)\n",
    "        print(MatureLat, MatureLon)\n",
    "    \n",
    "\n",
    "        if True:\n",
    "            datastruct[Month][IDs] = dict()\n",
    "            datastruct[Month][IDs]['clat'] = clat\n",
    "            datastruct[Month][IDs]['clon'] = clon\n",
    "            datastruct[Month][IDs]['zeta'] = slp            #zeta\n",
    "            # datastruct[Month][IDs]['hzeta'] = hourszeta     \n",
    "            datastruct[Month][IDs]['SLP'] = d[cappear,4]\n",
    "            datastruct[Month][IDs]['hSLP'] = hourstoSLPmin\n",
    "            datastruct[Month][IDs]['dates'] = dates\n",
    "            datastruct[Month][IDs]['Matlat'] = MatureLat\n",
    "            datastruct[Month][IDs]['Matlon'] = MatureLon\n",
    "\n",
    "        # ### these are preidentified tropical\n",
    "        # if(d[cappear[0],-3]==0):\n",
    "        #         label=0\n",
    "        # ### MED\n",
    "        # else:\n",
    "        #     for b,o in enumerate(LONM[:-1]):\n",
    "        #         if( ((30-MatureLat)<=0) & ((LATM[b]-MatureLat)>=0) &\n",
    "        #     ((o-MatureLon)<=0) & ((LONM[b+1]-MatureLon)>=0)):\n",
    "        #                 label=2\n",
    "        # ### ATL\n",
    "        #         else:\n",
    "        #             label=1\n",
    "\n",
    "        ### Max set the label to something\n",
    "        label = 0. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        datastruct[Month][IDs]['label'] = label\n",
    "\n",
    "\n",
    "PATH = '/net/helium/atmosdyn/freimax/data_msc/casestudy_ra/data/features/'\n",
    "full_path = os.path.join(PATH, MOT, add + '.txt')\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "\n",
    "# Now you can write the file\n",
    "with open(full_path, 'wb') as f:\n",
    "    pickle.dump(datastruct, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All-CYC-entire-year-NEWAPR17-correct'"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alex script continues here\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23599/2250215231.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if (~(np.any(Gdates==Date))):\n"
     ]
    }
   ],
   "source": [
    "###  restrict myself to Atlantic, tropical and Mediterranean ones\n",
    "for Month in months:\n",
    "    datastruct[Month] = dict()\n",
    "\n",
    "    if Month == 'DEC17':\n",
    "        track = dbase + '002-2020-08-05-Identify-DJF1718-medcyclones/' + Month  + '/TRACKED_CYCLONES'\n",
    "        d = np.loadtxt(track)\n",
    "\n",
    "    ### added by MAX\n",
    "    elif Month == 'APR17':\n",
    "        track = '/net/helium/atmosdyn/freimax/data_msc/casestudy_ra/data/features/TRACKED_CYCLONE.txt'\n",
    "        d = np.loadtxt(track, skiprows=1)\n",
    "    else:\n",
    "        track = '/home/atroman/phd/' + Month + '/features/tracking/TRACKED_CYCLONES'\n",
    "        d = np.loadtxt(track,skiprows=1)\n",
    "\n",
    "    IDs = np.unique(d[:,1])\n",
    "    for ids in IDs[:]:\n",
    "       cappear, = np.where((d[:,1]==ids))\n",
    "       indomainalready = 0\n",
    "       for lonb, latb in zip(LONSS,LATSS):\n",
    "        if indomainalready==1:\n",
    "            continue\n",
    "        #first & last & mid track point in boundary\n",
    "\n",
    "        if((d[cappear[0],2]>lonb[0]) & (d[cappear[0],2]<lonb[-1])):\n",
    "\n",
    "         if ((d[cappear[-1],2]>lonb[0]) & (d[cappear[-1],2]<lonb[-1])):\n",
    "\n",
    "          if ((d[cappear[int(len(cappear)/2)],2]>lonb[0]) & (d[cappear[int(len(cappear)/2)],2]<lonb[-1])):\n",
    "           CONTINUE=0\n",
    "\n",
    "           # check lat boundary\n",
    "           for k,l in enumerate(lonb[:-1]):\n",
    "                if ((d[cappear[0],2]>l) & (d[cappear[0],2]<lonb[k+1]) & (d[cappear[0],3]>latb[2*k]) &(d[cappear[0],3]<latb[2*k+1])):\n",
    "                    CONTINUE=1\n",
    "                    indomainalready=1\n",
    "           if (CONTINUE==1):\n",
    "            #all local data for particular cyclones\n",
    "#            clat = np.zeros((len(r200_lonids)),dtype=int)\n",
    "#            clon = np.zeros((len(r200_lonids)),dtype=int)\n",
    "            clat = []\n",
    "            clon = []\n",
    "            hourszeta = np.array([])\n",
    "            hourstoSLPmin = np.array([])\n",
    "            zetal = np.array([])\n",
    "            dates = np.array([])\n",
    "            zeta = np.zeros(len(cappear))\n",
    "            hours = np.zeros(len(cappear))\n",
    "\n",
    "            ### find id of minimum SLP in track file\n",
    "            slpminid = cappear[np.where(d[cappear,6] == np.min(d[cappear,6]))]\n",
    "            slpminid = slpminid[-1]\n",
    "            hourstoSLPmin = np.append(hourstoSLPmin, d[cappear,0]-d[slpminid,0])\n",
    "\n",
    "            ### step through hrs of the cyclon in trackfile\n",
    "            for wr,u in enumerate(cappear):\n",
    "                tmp = d[u]\n",
    "                k = str(helper.datenum_to_datetime(ft+tmp[0]/24))\n",
    "                Date = k[0:4]+k[5:7]+k[8:10]+'_'+k[11:13]\n",
    "\n",
    "                ### if that date has not been loaded yet, load it\n",
    "                if (~(np.any(Gdates==Date))):\n",
    "                    data[Date] = xr.open_dataset('/net/thermo/atmosdyn/atroman/phd/'+Month+'/cdf/S'+Date, drop_variables=['PV','P','TH','THE','RH','PVRCONVT','PVRCONVM','PVRTURBT','PVRTURBM','PVRLS','PVRCOND','PVRSW','PVRLWH','PVRLWC','PVRDEP','PVREVC','PVREVR','PVRSUBI','PVRSUBS','PVRMELTI','PVRMELTS','PVRFRZ','PVRRIME','PVRBF'])\n",
    "\n",
    "                dates = np.append(dates,Date)\n",
    "                Gdates = np.append(Gdates,Date)\n",
    "\n",
    "                #hours since first identification\n",
    "                hours[wr] = tmp[0]\n",
    "                ### keep adding lon to lon and lat to lat and use srtid, as it is correct with normal crosssections\n",
    "                CLONIDS, CLATIDS = helper.IFS_radial_ids_correct(200,LAT[np.where(LAT==np.round(tmp[3],1))[0][0]])\n",
    "                addlon = CLONIDS + np.where(LON==np.round(tmp[2],1))[0][0]\n",
    "                addlon[np.where((addlon-900)>0)] = addlon[np.where((addlon-900)>0)]-900\n",
    "\n",
    "                clat.append(CLATIDS.astype(int) + np.where(LAT==np.round(tmp[3],1))[0][0])\n",
    "                clon.append(addlon.astype(int))\n",
    "#                clat = np.vstack((clat, CLATIDS + np.where(LAT==np.round(tmp[3],1))[0][0]))\n",
    "#                clon = np.vstack((clon, addlon))\n",
    "\n",
    "                ### use last lon and lat entries [-1] of center\n",
    "                PS = data[Date].PS.values[0,0,clat[-1],clon[-1]]\n",
    "                for e in range(len(CLATIDS)):\n",
    "                    P = helper.modellevel_to_pressure(PS[e])\n",
    "                    I = np.where(abs(P-850)==np.min(abs(P-850)))[0]\n",
    "                    I = I[0].astype(int)\n",
    "                    zetal = np.append(zetal,data[Date].VORT.values[0,I,clat[-1][e],clon[-1][e]])\n",
    "\n",
    "                zeta[wr] = np.mean(zetal)\n",
    "\n",
    "            hours = hours - hours[np.where(zeta==np.max(zeta))]\n",
    "            hourszeta = np.append(hourszeta,hours)\n",
    "\n",
    "            # allow for at least 48h backward trajectories\n",
    "            if (((int(dates[np.where(zeta==np.max(zeta))[0][-1]][6:8])>=3)) | (int(dates[np.where(zeta==np.max(zeta))[0][-1]][4:6])!=MONTHN[np.where(MONTHS==MOT)[0][0]])):\n",
    "\n",
    "            ### added 30.10.2020 to ensure only MED cyclones\n",
    "            ### add 1 as clat, clon start with zeros as base to stack them\n",
    "\n",
    "             MatureLat = np.round(np.mean(LAT[clat[np.where(zeta==np.max(zeta))[0][-1]]]),1)\n",
    "             MatureLon = np.round(np.mean(LON[clon[np.where(zeta==np.max(zeta))[0][-1]]]),1)\n",
    "            # use latest maximum relative vorticity\n",
    "#             MatureLat = np.round(np.mean(LAT[clat[np.where(zeta==np.max(zeta))[0][-1]+1]]),1)\n",
    "#             MatureLon = np.round(np.mean(LON[clon[np.where(zeta==np.max(zeta))[0][-1]+1]]),1)\n",
    "            #if time to mature stage is too short remove from data\n",
    "             if((hours[0]<=(-2)) & (hours[-1]>=2)):\n",
    "                datastruct[Month][ids] = dict()\n",
    "                datastruct[Month][ids]['clat'] = clat\n",
    "                datastruct[Month][ids]['clon'] = clon\n",
    "                datastruct[Month][ids]['zeta'] = zeta\n",
    "                datastruct[Month][ids]['hzeta'] = hourszeta\n",
    "                datastruct[Month][ids]['SLP'] = d[cappear,6]\n",
    "                datastruct[Month][ids]['hSLP'] = hourstoSLPmin\n",
    "                datastruct[Month][ids]['dates'] = dates\n",
    "                datastruct[Month][ids]['Matlat'] = MatureLat\n",
    "                datastruct[Month][ids]['Matlon'] = MatureLon\n",
    "\n",
    "                ### these are preidentified tropical\n",
    "                if(d[cappear[0],-3]==0):\n",
    "                     label=0\n",
    "                ### MED\n",
    "                else:\n",
    "                    for b,o in enumerate(LONM[:-1]):\n",
    "                        if( ((30-MatureLat)<=0) & ((LATM[b]-MatureLat)>=0) &\n",
    "                   ((o-MatureLon)<=0) & ((LONM[b+1]-MatureLon)>=0)):\n",
    "                             label=2\n",
    "                ### ATL\n",
    "                        else:\n",
    "                         label=1\n",
    "                datastruct[Month][ids]['label'] = label\n",
    "\n",
    "# f = open(PATH + MOT + '/' + add + '.txt','wb')\n",
    "# pickle.dump(datastruct,f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maxpy3_2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

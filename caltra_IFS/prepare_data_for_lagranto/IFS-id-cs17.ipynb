{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ascherrmann/scripts/')\n",
    "import helper\n",
    "import argparse\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs from the TRACKED_CYCLONE file:\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      " 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n",
      " 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50.]\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################\n",
    "## This cell is just a test\n",
    "\n",
    "path_trackCYC = f'/net/thermo/atmosdyn/atroman/PAC1d/cdf/TRACKED_CYCLONES'\n",
    "\n",
    "\n",
    "# d = np.loadtxt(path_trackCYC,skiprows=1)\n",
    "d = pd.read_csv(path_trackCYC, skiprows=1, delim_whitespace=True, na_values='NA').values\n",
    "IDs = np.unique(d[:,1])\n",
    "# try:\n",
    "# except IndexError:\n",
    "#     IDs = np.unique(d[-1])\n",
    "print(f'IDs from the TRACKED_CYCLONE file:\\n{IDs}')\n",
    "######################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create File with cyclone information of case study 17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of d:\n",
    "-  0 col:    Time\n",
    "-  1 col:    ID\n",
    "-  2 col:    LON\n",
    "-  3 col:    LAT\n",
    "-  4 col:    Distance\n",
    "-  5 col:    Area\n",
    "-  6 col:    Inpres\n",
    "-  7 col:    Outpres\n",
    "-  8 col:    baroclin\n",
    "-  9 col:    meanth\n",
    "- 10 col:    ctype\n",
    "- 11 col:    bergeron\n",
    "- 12 col:    bmaxdiff\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise dictornary where important fields are saved\n",
    "datastruct = dict()\n",
    "\n",
    "labels = np.array([])\n",
    "Gdates = np.array([])\n",
    "data = dict()\n",
    "\n",
    "LON = np.round(np.linspace(-180,180,901),1)\n",
    "LAT = np.round(np.linspace(0,90,226),1)\n",
    "\n",
    "\n",
    "add = 'CaseStudy-CYC-Apr17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs from the TRACKED_CYCLONE file:\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      " 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n",
      " 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50.]\n"
     ]
    }
   ],
   "source": [
    "## LOAD DATA\n",
    "################################# MODIFIED BY MAX ############################################################################\n",
    "# - Load Cyclone Track file\n",
    "path_trackCYC = f'/net/thermo/atmosdyn/atroman/PAC1d/cdf/TRACKED_CYCLONES'\n",
    "d_all = pd.read_csv(path_trackCYC, skiprows=1, delim_whitespace=True, na_values='NA').values\n",
    "IDs = np.unique(d_all[:,1])\n",
    "print(f'IDs from the TRACKED_CYCLONE file:\\n{IDs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.89720000e+05, 1.90000000e+01, 1.63600000e+02, 4.76000000e+01,\n",
       "        2.63561711e+06, 2.87289299e+06, 9.65271851e+02, 1.00000000e+03,\n",
       "        4.38835144e+01, 2.87381882e+02, 5.78752470e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Only select case-study cyclone: \n",
    "# time: 20170410_17   \n",
    "# lon:  136.6\n",
    "# lat:  47.2\n",
    "\n",
    "## Filter for desired cyclone by lon/lat\n",
    "d=d_all[(d_all[:,2]>=163.4) & (d_all[:,2]<=163.8) & (d_all[:,3]>=0) & (d_all[:,3]<=90)]\n",
    "\n",
    "# row_min_slp = d[:,6].argmin() \n",
    "# d = d[row_min_slp]\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = 19# d[1]\n",
    "## cappear contains all indices of rows in d that have ids (one of the validation IDs)        \n",
    "cappear, = np.where((d[:,1]==ids))  # This is just 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cappear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "clat = []\n",
    "clon = []\n",
    "hourszeta = np.array([])\n",
    "hourstoSLPmin = np.array([])\n",
    "zetal = np.array([])\n",
    "dates = np.array([])\n",
    "zeta = np.zeros(len(cappear))\n",
    "hours = np.zeros(len(cappear))\n",
    "\n",
    "\n",
    "hoursslp = np.array([])         # MAX\n",
    "slpl = np.array([])             # MAX\n",
    "slp = np.zeros(len(cappear))    # MAX\n",
    "\n",
    "\n",
    "### find id of minimum SLP in track file\n",
    "slpminid = cappear[np.where(d[cappear,6] == np.min(d[cappear,6]))]   # NOTE: d[cappear,4] IS EQUAL TO d[:,4]   --> values of 5th column = \"inpres\"\n",
    "slpminid = slpminid[-1]                                              # Extract number from array\n",
    "hourstoSLPmin = np.append(hourstoSLPmin, d[cappear,0]-d[slpminid,0]) # Substract time from min SLP from other times to get seconds to/from SLP minimum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourstoSLPmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-04-10 16:00:00'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### step through hrs of the cyclon in trackfile\n",
    "# for wr,u in enumerate(cappear):                   ## Not necessary, since cappear is only 0, i.e. wr == u == 0\n",
    "wr,u = 0,0\n",
    "\n",
    "tmp = d[u]\n",
    "\n",
    "ft = date.toordinal(date(1950,1,1))\n",
    "k = str(helper.datenum_to_datetime(ft+tmp[0]/24))\n",
    "Date = k[0:4]+k[5:7]+k[8:10]+'_'+k[11:13]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates, Gdates = [],[]\n",
    "\n",
    "# Load dataset with PS, SLP and Vorticity\n",
    "dataset_1 = xr.open_dataset(f'/net/thermo/atmosdyn/atroman/PAC1d/cdf/2017/04/S{Date}', drop_variables=['PV','P','TH','THE','RH','PVRCONVT','PVRCONVM','PVRTURBT','PVRTURBM','PVRLS','PVRCOND','PVRSW','PVRLWH','PVRLWC','PVRDEP','PVREVC','PVREVR','PVRSUBI','PVRSUBS','PVRMELTI','PVRMELTS','PVRFRZ','PVRRIME','PVRBF'])\n",
    "data[Date] = dataset_1\n",
    "\n",
    "dates = np.append(dates,Date)\n",
    "Gdates = np.append(Gdates,Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hours since first identification\n",
    "hours[wr] = tmp[0]\n",
    "### keep adding lon to lon and lat to lat and use srtid, as it is correct with normal crosssections\n",
    "CLONIDS, CLATIDS = helper.IFS_radial_ids_correct(200,LAT[np.where(LAT==np.round(tmp[3],1))[0][0]])\n",
    "addlon = CLONIDS + np.where(LON==np.round(tmp[2],1))[0][0]\n",
    "addlon[np.where((addlon-900)>0)] = addlon[np.where((addlon-900)>0)]-900\n",
    "\n",
    "clat.append(CLATIDS.astype(int) + np.where(LAT==np.round(tmp[3],1))[0][0])\n",
    "clon.append(addlon.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "### use last lon and lat entries [-1] of center\n",
    "PS = data[Date].PS.values[0,0,clat[-1],clon[-1]]\n",
    "for e in range(len(CLATIDS)):\n",
    "    P = helper.modellevel_to_pressure(PS[e])\n",
    "    I = np.where(abs(P-850)==np.min(abs(P-850)))[0]\n",
    "    I = I[0].astype(int)\n",
    "    zetal = np.append(zetal,data[Date].VORT.values[0,I,clat[-1][e],clon[-1][e]])\n",
    "\n",
    "zeta[wr] = np.mean(zetal)\n",
    "\n",
    "hours = hours - hours[np.where(zeta==np.max(zeta))]\n",
    "hourszeta = np.append(hourszeta,hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "MatureLat = np.round(np.mean(LAT[clat[np.where(zeta==np.max(zeta))[0][-1]]]),1)\n",
    "MatureLon = np.round(np.mean(LON[clon[np.where(zeta==np.max(zeta))[0][-1]]]),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cyclone ID: 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#if time to mature stage is too short remove from data\n",
    "# if((hours[0]<=(-2)) & (hours[-1]>=2)):        # ALEX\n",
    "if((hours[0]<=(-0)) & (hours[-1]>=0)):          # MAX\n",
    "    datastruct[ids] = dict()\n",
    "    datastruct[ids]['clat'] = clat\n",
    "    datastruct[ids]['clon'] = clon\n",
    "    datastruct[ids]['zeta'] = zeta\n",
    "    datastruct[ids]['hzeta'] = hourszeta\n",
    "    datastruct[ids]['SLP'] = d[cappear,6]\n",
    "    datastruct[ids]['hSLP'] = hourstoSLPmin\n",
    "    datastruct[ids]['dates'] = dates\n",
    "    datastruct[ids]['Matlat'] = MatureLat\n",
    "    datastruct[ids]['Matlon'] = MatureLon\n",
    "    # ### these are preidentified tropical\n",
    "    # if(d[cappear[0],-3]==0):\n",
    "    #         label=0\n",
    "    # ### MED\n",
    "    # else:\n",
    "    #     for b,o in enumerate(LONM[:-1]):\n",
    "    #         if( ((30-MatureLat)<=0) & ((LATM[b]-MatureLat)>=0) & ((o-MatureLon)<=0) & ((LONM[b+1]-MatureLon)>=0)):\n",
    "    #                 label=2\n",
    "    # ### ATL\n",
    "    #         else:\n",
    "    #             label=1\n",
    "    datastruct[ids]['label'] = ids\n",
    "    print('Saved cyclone ID:', ids)\n",
    "\n",
    "else: \n",
    "    print(f'File not saved, becuae time to mature stage is not long enough: {hours[0]}, {hours[-1]} ')\n",
    "\n",
    "PATH = '/net/helium/atmosdyn/freimax/data_msc/IFS-17/ctraj'\n",
    "# Create the directory if it does not exist\n",
    "\n",
    "f = open(f'{PATH}/{add}.txt','wb')\n",
    "pickle.dump(datastruct,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maxpy3_2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
